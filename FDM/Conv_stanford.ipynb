{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectralDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Builds a dataset of spectral data. Use idxs to specify which samples to use\n",
    "    for dataset - this allows for random splitting into training, validation,\n",
    "    and test sets. Instead of passing in filenames for X and y, we can also\n",
    "    pass in numpy arrays directly.\n",
    "    \"\"\"\n",
    "    def __init__(self, X_fn, y_fn, idxs=None, transform=None):\n",
    "        if type(X_fn) == str:\n",
    "            self.X = np.load(X_fn)\n",
    "        else:\n",
    "            self.X = X_fn\n",
    "        if type(y_fn) == str:\n",
    "            self.y = np.load(y_fn)\n",
    "        else:\n",
    "            self.y = y_fn\n",
    "        if idxs is None: idxs = np.arange(len(self.y))\n",
    "        self.idxs = idxs\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i = self.idxs[idx]\n",
    "        x, y = self.X[i], self.y[i]\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return (x, y)\n",
    "\n",
    "\n",
    "### TRANSFORMS ###\n",
    "\n",
    "\n",
    "class GetInterval(object):\n",
    "    \"\"\"\n",
    "    Gets an interval of each spectrum.\n",
    "    \"\"\"\n",
    "    def __init__(self, min_idx, max_idx):\n",
    "        self.min_idx = min_idx\n",
    "        self.max_idx = max_idx\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = x[:,self.min_idx:self.max_idx]\n",
    "        return x\n",
    "\n",
    "\n",
    "class ToFloatTensor(object):\n",
    "    \"\"\"\n",
    "    Converts numpy arrays to float Variables in Pytorch.\n",
    "    \"\"\"\n",
    "    def __call__(self, x):\n",
    "        x = torch.from_numpy(x).float()\n",
    "        return x\n",
    "\n",
    "\n",
    "### TRANSFORMS ###\n",
    "\n",
    "\n",
    "def spectral_dataloader(X_fn, y_fn, idxs=None, batch_size=10, shuffle=True,\n",
    "    num_workers=0, min_idx=None, max_idx=None, sampler=None):\n",
    "    \"\"\"\n",
    "    Returns a DataLoader with spectral data.\n",
    "    \"\"\"\n",
    "    transform_list = []\n",
    "    if min_idx is not None and max_idx is not None:\n",
    "        transform_list.append(GetInterval(min_idx, max_idx))\n",
    "    transform_list.append(ToFloatTensor())\n",
    "    transform = transforms.Compose(transform_list)\n",
    "    dataset = SpectralDataset(X_fn, y_fn, idxs=idxs, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle,\n",
    "        num_workers=num_workers, sampler=sampler)\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "def spectral_dataloaders(X_fn, y_fn, n_train=None, p_train=0.8, p_val=0.1,\n",
    "    n_test=None, batch_size=10, shuffle=True, num_workers=0, min_idx=None,\n",
    "    max_idx=None):\n",
    "    \"\"\"\n",
    "    Returns train, val, and test DataLoaders by splitting the dataset randomly.\n",
    "    Can also take X_fn and y_fn as numpy arrays.\n",
    "    \"\"\"\n",
    "    if type(y_fn) == str:\n",
    "        idxs = np.arange(len(np.load(y_fn)))\n",
    "    else:\n",
    "        idxs = np.arange(len(y_fn))\n",
    "    np.random.shuffle(idxs)\n",
    "    if n_train is None: n_train = int(p_train * len(idxs))\n",
    "    n_val = int(p_val * n_train)\n",
    "    val_idxs, train_idxs = idxs[:n_val], idxs[n_val:n_train]\n",
    "    if n_test is None: test_idxs = idxs[n_train:]\n",
    "    else: test_idxs = idxs[n_train:n_train+n_test]\n",
    "    trainloader = spectral_dataloader(X_fn, y_fn, train_idxs,\n",
    "        batch_size=batch_size, shuffle=shuffle, num_workers=num_workers,\n",
    "        min_idx=min_idx, max_idx=max_idx)\n",
    "    valloader = spectral_dataloader(X_fn, y_fn, val_idxs,\n",
    "        batch_size=batch_size, shuffle=shuffle, num_workers=num_workers,\n",
    "        min_idx=min_idx, max_idx=max_idx)\n",
    "    testloader = spectral_dataloader(X_fn, y_fn, test_idxs,\n",
    "        batch_size=batch_size, shuffle=shuffle, num_workers=num_workers,\n",
    "        min_idx=min_idx, max_idx=max_idx)\n",
    "    return (trainloader, valloader, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'B:/bacteria_id_data/data/unzipped_data/X_finetune.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-52764f237759>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mX_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'B:/bacteria_id_data/data/unzipped_data/X_finetune.npy'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0my_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'B:/bacteria_id_data/data/unzipped_data/y_finetune.npy'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    426\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 428\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    429\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'B:/bacteria_id_data/data/unzipped_data/X_finetune.npy'"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "t00 = time()\n",
    "import numpy as np\n",
    "\n",
    "X_fn = 'B:/bacteria_id_data/data/X_finetune.npy'\n",
    "y_fn = 'B:/bacteria_id_data/data/y_finetune.npy'\n",
    "X = np.load(X_fn)\n",
    "y = np.load(y_fn)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN parameters\n",
    "layers = 6\n",
    "hidden_size = 100\n",
    "block_size = 2\n",
    "hidden_sizes = [hidden_size] * layers\n",
    "num_blocks = [block_size] * layers\n",
    "input_dim = 1000\n",
    "in_channels = 64\n",
    "n_classes = 30\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '{}'.format(0)\n",
    "cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        # Layers\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=5,\n",
    "            stride=stride, padding=2, dilation=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=out_channels)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=5,\n",
    "            stride=1, padding=2, dilation=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(num_features=out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=1,\n",
    "                    stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(out_channels))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, hidden_sizes, num_blocks, input_dim=1000,\n",
    "        in_channels=64, n_classes=30):\n",
    "        super(ResNet, self).__init__()\n",
    "        assert len(num_blocks) == len(hidden_sizes)\n",
    "        self.input_dim = input_dim\n",
    "        self.in_channels = in_channels\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(1, self.in_channels, kernel_size=5, stride=1,\n",
    "            padding=2, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(self.in_channels)\n",
    "        \n",
    "        # Flexible number of residual encoding layers\n",
    "        layers = []\n",
    "        strides = [1] + [2] * (len(hidden_sizes) - 1)\n",
    "        for idx, hidden_size in enumerate(hidden_sizes):\n",
    "            layers.append(self._make_layer(hidden_size, num_blocks[idx],\n",
    "                stride=strides[idx]))\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "\n",
    "        self.z_dim = self._get_encoding_size()\n",
    "        self.linear = nn.Linear(self.z_dim, self.n_classes)\n",
    "\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.encoder(x)\n",
    "        z = x.view(x.size(0), -1)\n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encode(x)\n",
    "        return self.linear(z)\n",
    "\n",
    "\n",
    "    def _make_layer(self, out_channels, num_blocks, stride=1):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        blocks = []\n",
    "        for stride in strides:\n",
    "            blocks.append(ResidualBlock(self.in_channels, out_channels,\n",
    "                stride=stride))\n",
    "            self.in_channels = out_channels\n",
    "        return nn.Sequential(*blocks)\n",
    "\n",
    "    def _get_encoding_size(self):\n",
    "        \"\"\"\n",
    "        Returns the dimension of the encoded input.\n",
    "        \"\"\"\n",
    "        temp = Variable(torch.rand(1, 1, self.input_dim))\n",
    "        z = self.encode(temp)\n",
    "        z_dim = z.data.size(1)\n",
    "        return z_dim\n",
    "\n",
    "\n",
    "def add_activation(activation='relu'):\n",
    "    \"\"\"\n",
    "    Adds specified activation layer, choices include:\n",
    "    - 'relu'\n",
    "    - 'elu' (alpha)\n",
    "    - 'selu'\n",
    "    - 'leaky relu' (negative_slope)\n",
    "    - 'sigmoid'\n",
    "    - 'tanh'\n",
    "    - 'softplus' (beta, threshold)\n",
    "    \"\"\"\n",
    "    if activation == 'relu':\n",
    "        return nn.ReLU()\n",
    "    elif activation == 'elu':\n",
    "        return nn.ELU(alpha=1.0)\n",
    "    elif activation == 'selu':\n",
    "        return nn.SELU()\n",
    "    elif activation == 'leaky relu':\n",
    "        return nn.LeakyReLU(negative_slope=0.1)\n",
    "    elif activation == 'sigmoid':\n",
    "        return nn.Sigmoid()\n",
    "    elif activation == 'tanh':\n",
    "        return nn.Tanh()\n",
    "    # SOFTPLUS DOESN'T WORK with automatic differentiation in pytorch\n",
    "    elif activation == 'softplus':\n",
    "        return nn.Softplus(beta=1, threshold=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load trained weights for demo\n",
    "cnn = ResNet(hidden_sizes, num_blocks, input_dim=input_dim,\n",
    "                in_channels=in_channels, n_classes=n_classes)\n",
    "if cuda: cnn.cuda()\n",
    "cnn.load_state_dict(torch.load(\n",
    "    'B:/bacteria_id_data/pretrained_model.ckpt', map_location=lambda storage, loc: storage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def run_epoch(epoch, model, dataloader, cuda, training=False, optimizer=None):\n",
    "    if training:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "        if cuda: inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = Variable(inputs), Variable(targets.long())\n",
    "        outputs = model(inputs)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, targets)\n",
    "        if training:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        if cuda:\n",
    "            correct += predicted.eq(targets.data).cpu().sum().item()\n",
    "        else:\n",
    "            correct += predicted.eq(targets.data).sum().item()\n",
    "    acc = 100 * correct / total\n",
    "    avg_loss = total_loss / total\n",
    "    return acc, avg_loss\n",
    "\n",
    "\n",
    "def get_predictions(model, dataloader, cuda, get_probs=False):\n",
    "    preds = []\n",
    "    model.eval()\n",
    "    for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "        if cuda: inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = Variable(inputs), Variable(targets.long())\n",
    "        outputs = model(inputs)\n",
    "        if get_probs:\n",
    "            probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "            if cuda: probs = probs.data.cpu().numpy()\n",
    "            else: probs = probs.data.numpy()\n",
    "            preds.append(probs)\n",
    "        else:\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            if cuda: predicted = predicted.cpu()\n",
    "            preds += list(predicted.numpy().ravel())\n",
    "    if get_probs:\n",
    "        return np.vstack(preds)\n",
    "    else:\n",
    "        return np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_val = 0.1\n",
    "n_val = int(3000 * p_val)\n",
    "idx_tr = list(range(3000))\n",
    "np.random.shuffle(idx_tr)\n",
    "idx_val = idx_tr[:n_val]\n",
    "idx_tr = idx_tr[n_val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fine-tuning!\n",
      " Epoch 1: 0.00s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-efec5989a36d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m# Train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     acc_tr, loss_tr = run_epoch(epoch, cnn, dl_tr, cuda,\n\u001b[1;32m---> 21\u001b[1;33m         training=True, optimizer=optimizer)\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'  Train acc: {:0.2f}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc_tr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;31m# Val\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-70-70862ff9aa35>\u001b[0m in \u001b[0;36mrun_epoch\u001b[1;34m(epoch, model, dataloader, cuda, training, optimizer)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \"\"\"\n\u001b[1;32m--> 107\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Fine-tune CNN\n",
    "epochs = 1 # Change this number to ~30 for full training\n",
    "batch_size = 10\n",
    "t0 = time()\n",
    "# Set up Adam optimizer\n",
    "optimizer = optim.Adam(cnn.parameters(), lr=1e-3, betas=(0.5, 0.999))\n",
    "# Set up dataloaders\n",
    "dl_tr = spectral_dataloader(X, y, idxs=idx_tr,\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "dl_val = spectral_dataloader(X, y, idxs=idx_val,\n",
    "    batch_size=batch_size, shuffle=False)\n",
    "# Fine-tune CNN for first fold\n",
    "best_val = 0\n",
    "no_improvement = 0\n",
    "max_no_improvement = 5\n",
    "print('Starting fine-tuning!')\n",
    "for epoch in range(epochs):\n",
    "    print(' Epoch {}: {:0.2f}s'.format(epoch+1, time()-t0))\n",
    "    # Train\n",
    "    acc_tr, loss_tr = run_epoch(epoch, cnn, dl_tr, cuda,\n",
    "        training=True, optimizer=optimizer)\n",
    "    print('  Train acc: {:0.2f}'.format(acc_tr))\n",
    "    # Val\n",
    "    acc_val, loss_val = run_epoch(epoch, cnn, dl_val, cuda,\n",
    "        training=False, optimizer=optimizer)\n",
    "    print('  Val acc  : {:0.2f}'.format(acc_val))\n",
    "    # Check performance for early stopping\n",
    "    if acc_val > best_val or epoch == 0:\n",
    "        best_val = acc_val\n",
    "        no_improvement = 0\n",
    "    else:\n",
    "        no_improvement += 1\n",
    "    if no_improvement >= max_no_improvement:\n",
    "        print('Finished after {} epochs!'.format(epoch+1))\n",
    "        break\n",
    "\n",
    "print('\\n This demo was completed in: {:0.2f}s'.format(time()-t00))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
